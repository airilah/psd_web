[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Klasifikasi Kanker Payudara Menggunakan Model Random Forest",
    "section": "",
    "text": "Preface\nThis is a Quarto book."
  },
  {
    "objectID": "index.html#software-conventions",
    "href": "index.html#software-conventions",
    "title": "Klasifikasi Kanker Payudara Menggunakan Model Random Forest",
    "section": "Software conventions",
    "text": "Software conventions\n\n1 + 1\n\n2\n\n\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Klasifikasi Kanker Payudara Menggunakan Model Random Forest",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nBlah, blah, blah…"
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "",
    "text": "2 1. BUSSINESS UNDERSTANDING\nKlasifikasi Dataset Kanker Payudara\ntujuannya: untuk membangun model klasifikasi yang dapat memprediksi apakah seorang terdiagnosis kanker payudara ganas atau jinak. karena ketika seorang hendak untuk mengetahui apakah seorang tersebut terdiagnosis kanker payudara maka dengan melakukan pengecekan pada setiap atribut yang ada di data maka orang tersebut akan tau apakah dia terdiagnosis kanker atau tidak. jadi jika seorang tersebut secepatnya tau bahwa dia terdiagnosis kanker payudara maka pengobatan akan segera di lakukan. hal ini yang membantu meningkatkan keselamatan pasien.\npenjelasan setiap fitur: - Age (Usia): Ini adalah usia pasien dalam tahun. (tahun/year)\nDataset berupa kumpulan data test Kanker Payudara yang dari beberapa pasien. dataset diperoleh dari website UC Irvine Machine Learning Repository dan di upload pada 3 Mei 2018. Terdapat 10 prediktor, semuanya kuantitatif, dan variabel dependen biner, yang menunjukkan ada tidaknya kanker payudara. Prediktornya adalah data antropometri dan parameter yang dapat dikumpulkan dalam analisis darah rutin. Model prediksi berdasarkan prediktor tersebut, jika akurat, berpotensi digunakan sebagai biomarker kanker payudara.\nAdapun hal - hal yang perlu dilakukan untuk memahami data, yakni 1. Mendeskripsikan setiap fitur pada data * tipe data * deskripsi data 2. Mengidentifikasi missing values setiap fitur atau kolom 3. Eksplorasi data (grafikan fitur) 4. Mengidentifikasi outlier 5. Mengidentifikasi jumlah data (proporsi data perkelas -untuk mengetahui balancing dataset atau keseimbangan data per kelas)\nSetelah memahami data, akan dilakukan tahap preprocessing untuk menangani masalah pada data yang sudah didefinisikan pada data understanding, yakni: 1. Menghapus Data Duplikat 2. Menghapus Outlier\nSetelah data siap, akan dilakukan : 1. Skoring tiap fitur kembali 2. Normalisasi Data 3. Eksplorasi Model\ncode dilanjut pada file main.py untuk membangun sistem"
  },
  {
    "objectID": "main.html#load-dataset",
    "href": "main.html#load-dataset",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "3.1 Load Dataset",
    "text": "3.1 Load Dataset\nmeng import/memanggil dataset kanker payudara\n\nimport pandas as pd\n\ndata = pd.read_excel('kankerpayudara.xlsx')\ndata.head(5)\n\n\n\n\n\n\n\n\nAge\nBMI\nGlucose\nInsulin\nHOMA\nLeptin\nAdiponectin\nResistin\nMCP.1\nClassification\n\n\n\n\n0\n48\n23.500000\n70\n2.707\n0.467409\n8.8071\n9.702400\n7.99585\n417.114\n1\n\n\n1\n83\n20.690495\n92\n3.115\n0.706897\n8.8438\n5.429285\n4.06405\n468.786\n1\n\n\n2\n82\n23.124670\n91\n4.498\n1.009651\n17.9393\n22.432040\n9.27715\n554.697\n1\n\n\n3\n68\n21.367521\n77\n3.226\n0.612725\n9.8827\n7.169560\n12.76600\n928.220\n1\n\n\n4\n86\n21.111111\n92\n3.549\n0.805386\n6.6994\n4.819240\n10.57635\n773.920\n1\n\n\n\n\n\n\n\n\n# Rincian dataset (banyak data dan kolom)\n\nprint(\"Banyaknya data : \", data.shape[0])\nprint(\"Banyaknya kolom : \", data.shape[1])\n\nBanyaknya data :  116\nBanyaknya kolom :  10\n\n\n\nkelas_counts = data['Classification'].value_counts()\nprint(kelas_counts)\n\nClassification\n2    64\n1    52\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\n\ndata.hist(figsize=(10, 8))\nplt.suptitle('Histograms of Features', x=0.5, y=0.95, ha='center', fontsize='x-large')\nplt.show()"
  },
  {
    "objectID": "main.html#mendeskripsikan-setiap-fitur",
    "href": "main.html#mendeskripsikan-setiap-fitur",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "3.2 Mendeskripsikan setiap fitur",
    "text": "3.2 Mendeskripsikan setiap fitur\nmendeskripsikan fitur apa saja yang ada pada fitur dataset kanker payudara\n\ndata.columns\n\nIndex(['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin', 'Adiponectin',\n       'Resistin', 'MCP.1', 'Classification'],\n      dtype='object')\n\n\n\n3.2.1 Tipe data\n\ndata.dtypes\n\nAge                 int64\nBMI               float64\nGlucose             int64\nInsulin           float64\nHOMA              float64\nLeptin            float64\nAdiponectin       float64\nResistin          float64\nMCP.1             float64\nClassification      int64\ndtype: object\n\n\nBerikut Macam - Macam Data yang ada pada data ini.\n\nTipe nominal\n\nmemiliki value 1 yang melambangkan ya dan 0 yang melambangkan tidak. &gt; Pada data ini mencakup fitur : ‘Classification’\nmencakup tipe data numeric. &gt; yakni pada fitur ‘Age’, ‘BMI’, ‘Glucose’, ‘Insulin’, ‘HOMA’, ‘Leptin’, ‘Adiponectin’, ‘Resistin’, ‘MCP.1’\n\n\n\n\n3.2.2 Deskripsi data\npenjelasan setiap fitur: - Age (Usia): Ini adalah usia pasien dalam tahun. (tahun/year)\n\nBMI (Body Mass Index): Ini adalah indeks massa tubuh (BMI) pasien, yang mengukur hubungan antara berat badan dan tinggi badan. Nilai ini digunakan untuk mengevaluasi status berat badan pasien. (Kg/m^2)\n\nKurang dari 18,5 = badan kurus/kurang\n18,5 - 22,9 = badan ideal/normal\n23 - 29,9 = badan gemuk/berlebih (cenderung obesitas)\nLebih dari 30 = obesitas\nUntuk cara pengukuran dilakukan (berat badan)/(tinggi badan)^2\n\nGlucose (Glukosa): Ini adalah konsentrasi glukosa dalam darah pasien, yang diukur dalam mg/dL. Kadar glukosa darah sering digunakan untuk mengawasi fungsi metabolisme gula dalam tubuh.(Mg/dl)\n\nlebih dari 100 mg/dL akurasi bisa berbeda ± 15 mg/dL\nkurang dari 100 mg/dL akurasi bisa berbeda ± 15%\n\nInsulin: Ini adalah kadar insulin dalam darah pasien, yang diukur dalam μU/mL. Insulin adalah hormon yang berperan dalam mengendalikan kadar glukosa darah.(µU/Ml)\nHOMA (Homeostasis Model Assessment): Ini adalah nilai HOMA yang digunakan untuk mengukur resistensi insulin dan fungsi sel beta pankreas dalam menghasilkan insulin. HOMA adalah perkiraan berdasarkan kadar glukosa dan insulin dalam darah. (%)\nLeptin: Leptin adalah hormon yang diproduksi oleh sel lemak dalam tubuh. Konsentrasi leptin dalam darah dapat berhubungan dengan berat badan dan metabolisme lemak. (ng/Ml)\nAdiponectin: Adiponectin adalah hormon yang diproduksi oleh jaringan lemak dan berperan dalam regulasi metabolisme lemak dan sensitivitas insulin. (µg/Ml)\nResistin: Resistin adalah protein yang diproduksi oleh jaringan lemak dan berperan dalam regulasi peradangan dan resistensi insulin. (ng/Ml)\nMCP.1 (Monocyte Chemoattractant Protein-1): MCP.1 adalah protein yang berperan dalam mengarahkan sel darah putih (monosit) ke daerah peradangan dalam tubuh. Ini dapat menjadi indikator peradangan dalam tubuh. (pg/dl)\nClassification (Klasifikasi): Ini adalah atribut target yang digunakan untuk mengklasifikasikan pasien. (1: Jinak, 2: Ganas)"
  },
  {
    "objectID": "main.html#mengidentifikasi-missing-value",
    "href": "main.html#mengidentifikasi-missing-value",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "3.3 Mengidentifikasi missing value",
    "text": "3.3 Mengidentifikasi missing value\n\n3.3.1 Missing value\nmencari missing value atau data yang tidak bernilai pada dataset kanker payudara\n\ndata.isna(): Fungsi ini menghasilkan DataFrame yang memiliki struktur yang sama dengan data, tetapi dengan nilai boolean (True atau False) yang menunjukkan apakah setiap sel dalam DataFrame data adalah nilai yang hilang atau tidak. Nilai True menunjukkan bahwa sel tersebut merupakan nilai yang hilang, sedangkan nilai False menunjukkan bahwa sel tersebut memiliki nilai.\n.any(): Metode ini kemudian digunakan untuk menerapkan fungsi any ke setiap kolom DataFrame hasil dari data.isna(). Ini menghasilkan Seri (Series) dengan indeks berupa nama kolom, dan nilai True atau False untuk setiap kolom, menunjukkan apakah kolom tersebut memiliki setidaknya satu nilai yang hilang atau tidak.\n\n\n# Menghitung apakah ada nilai yang hilang dalam setiap kolom\nmissing_values = data.isna().any()\n\n# Menampilkan hasil\nprint(\"Apakah ada nilai yang hilang dalam setiap kolom:\")\nprint(missing_values)\n\nApakah ada nilai yang hilang dalam setiap kolom:\nAge               False\nBMI               False\nGlucose           False\nInsulin           False\nHOMA              False\nLeptin            False\nAdiponectin       False\nResistin          False\nMCP.1             False\nClassification    False\ndtype: bool\n\n\nNoted : tidak ada missing value pada data\n\n\n3.3.2 Duplikat data\nmencari duplikat data yang terjadi pada dataset kanker payudara dengan menggunakan\n\ndata.duplicated(): Fungsi ini digunakan untuk menghasilkan serangkaian nilai boolean yang menunjukkan apakah setiap baris dalam DataFrame data adalah duplikat atau tidak. Nilai True menunjukkan bahwa baris tersebut merupakan duplikat, sementara nilai False menunjukkan bahwa baris tersebut tidak duplikat.\n.sum(): Metode ini kemudian digunakan untuk menjumlahkan nilai-nilai boolean yang dihasilkan oleh duplicated(). Jika suatu baris adalah duplikat, nilai booleannya adalah True yang dihitung sebagai 1, dan jika bukan duplikat, nilai booleannya adalah False yang dihitung sebagai 0.\n\n\njumlah_duplikat = data.duplicated().sum()\n\n# Menampilkan jumlah data yang duplikat\nprint(\"Jumlah data yang duplikat:\", jumlah_duplikat)\n\nJumlah data yang duplikat: 0\n\n\nNoted : terdapat beberapa baris data yang sama, sehingga data tersebut harus dihilangkan untuk menghindari adanya data yang redundan"
  },
  {
    "objectID": "main.html#mengidentifikasi-outlier",
    "href": "main.html#mengidentifikasi-outlier",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "3.4 Mengidentifikasi Outlier",
    "text": "3.4 Mengidentifikasi Outlier\nMencari data outlier pada dataset adalah langkah penting dalam analisis data untuk mengidentifikasi nilai-nilai yang berbeda secara signifikan dari pola umum dataset. Outlier adalah nilai yang jauh dari nilai-nilai lainnya dan dapat memiliki dampak besar pada hasil analisis statistik. Pada dataset kali ini saya menggunakan Local Outlier Factor untuk mencari outlier yang ada pada dataset kanker payudara tersebut.\n\n3.4.1 Local Outlier Factor\n\nPengertian Local Outlier Factor(LOF):\n\nLOF mengukur sejauh mana suatu observasi berbeda dari tetangga-tetangganya dalam hal kepadatan. Outlier diidentifikasi berdasarkan perbandingan antara kepadatan observasi tersebut dan kepadatan tetangganya. Jika suatu observasi memiliki LOF yang tinggi, maka itu dianggap sebagai outlier.\n\nLangkah-langkah Local Outlier Factor (LOF):\n\n\nHitung Jarak Antar Data dimana jarak yang dihitung adalah jarak titik yang akan dievaluasi dengan semua titik didalam satu baris. Perhitungan Jarak dilakukan menggunakan perhitungan jarak euclidean.\n\\[\n\\text{distance}(p, q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\n\\] dimana :\n\np = titik yang akan dievaluasi\nq = titik selain titik p\n\nHitung Kepadatan Lokal Setelah jarak diketahui, maka selanjutnya kepadatan lokal dari titik data tersebut perlu dihitung. Kepadatan lokal dapat dihitung dengan membandingkan jumlah titik-titik tetangga dalam jarak tertentu (radius) terhadap titik data yang sedang dievaluasi.\n\\[\n\\text{Local Density}(p) = \\frac{\\text{jumlah tetangga dalam radius}}{\\text{jumlah total data}}\n\\]\nHitung Local Reachability Density(LRD) Hitung kepadatan jarak (reachability distance) dari titik data (p) terhadap tetangganya (q). Local Reachability Density dari titik p terhadap tetangga q dihitung sebagai rata-rata dari jarak antara q dan p terhadap tetangga q:\n\\[\n\\text{reachdist}(p, q) = \\max(\\text{distance}(p, q), \\text{radius})\n\\]\n\\[\n\\text{Local Reachability Density}(p) = \\frac{1}{\\text{jumlah tetangga}} \\sum_{q \\in N_{\\text{radius}}(p)} \\frac{\\text{reachdist}(p, q)}{\\text{density}(q)}\n\\] dimana:\n\nN radius(p) adalah himpunan tetangga dalam radius tertentu radius dari titik p.\ndensity(q) adalah kepadatan lokal dari tetangga q.\n\nHitung Nilai LOF LOF dari suatu titik data (p) dihitung sebagai rasio dari rata-rata Local Reachability Density dari tetangganya terhadap kepadatan lokalnya sendiri: \\[\n\\text{LOF}(p) = \\frac{1}{\\text{jumlah tetangga}} \\sum_{q \\in N_{\\text{radius}}(p)} \\frac{\\text{Local Reachability Density}(q)}{\\text{Local Reachability Density}(p)}\n\\]\n\n\ncontoh kasus penggunaan outlier\n\n\n\n\nX\nY\n\n\n\n\n2\n6\n\n\n4\n7\n\n\n6\n9\n\n\n8\n5\n\n\n10\n12\n\n\n\nSekarang, kita akan mengikuti langkah-langkah yang sama untuk menghitung Local Outlier Factor (LOF):\nLangkah 1: Hitung Jarak Antar Data dengan Radius 5 | X | Y | Jarak | |—-|—-|——————| | 2 | 6 | 3.16 ; 3.61 ; 4.24| | 4 | 7 | 2.24 ; 2.83 ; 3.61| | 6 | 9 | 2.24 ; 3.61 ; 4.47| | 8 | 5 | 4.47 ; 5.1 | | 10 | 12 | 3.61 ; 5.83 |\nLangkah 2: Hitung Jumlah Tetangga dalam Radius 5 | X | Y | Jumlah Tetangga | | — | — | ————— | | 2 | 6 | 3 | | 4 | 7 | 3 | | 6 | 9 | 3 | | 8 | 5 | 2 | | 10 | 12 | 2 |\nLangkah 3: Hitung Local Reachability Density | X | Y | Jarak | |—-|—-|——————————————-| | 2 | 6 | (3.16 + 3.61 + 4.24) / 3 = 3.67 | | 4 | 7 | (2.24 + 2.83 + 3.61) / 3 = 2.89 | | 6 | 9 | (2.24 + 3.61 + 4.47) / 3 = 3.44 | | 8 | 5 | (4.47 + 5.1) / 2 = 4.79 | | 10 | 12 | (3.61 + 5.83) / 2 = 4.72 |\nLangkah 4: Menghitung Nilai LOF Data | X | Y | LOF | |—-|—-|———————–| | 2 | 6 | 1.28 | | 4 | 7 | 1.59 | | 6 | 9 | 1.20 | | 8 | 5 | 0.93 | | 10 | 12 | 0.84 |\nDengan begitu, nilai yang kemungkinan menjadi outlier adalah baris 4 dan baris 5, karena nilai LOF-nya lebih rendah dari 1, yang menunjukkan bahwa kepadatan lokal titik tersebut lebih tinggi daripada rata-rata kepadatan lokal tetangganya.\n\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n# Membuat model LOF\nclf = LocalOutlierFactor(n_neighbors=20)  # Jumlah tetangga yang digunakan\noutlier_scores = clf.fit_predict(data)\n\n# Menampilkan indeks outlier\noutlier_indices = np.where(outlier_scores == -1)[0]\nprint(\"Indeks outlier:\",outlier_indices)\nprint(\"Indeks outlier:\",len( outlier_indices))\n\nIndeks outlier: [  6  15  24  27  50  78  84  85  86  87  88 115]\nIndeks outlier: 12\n\n\nNoted : terdapat banyak data yang memiliki outlier, sehingga data tersebut harus dihilangkan"
  },
  {
    "objectID": "main.html#mengidentifikasi-jumlah-data",
    "href": "main.html#mengidentifikasi-jumlah-data",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "3.5 Mengidentifikasi Jumlah Data",
    "text": "3.5 Mengidentifikasi Jumlah Data\nuntuk mengindentifikasi jumlah data pada fitur classification atau data target pada dataset kanker payudara - value_counts(): Ini adalah metode dari pandas yang digunakan untuk menghitung frekuensi kemunculan setiap nilai unik dalam kolom ‘Classification’. Dengan kata lain, ini menghitung berapa kali setiap nilai muncul dalam kolom tersebut.\n\ntarget_no_outliers = data['Classification'].value_counts()\n\n# Menampilkan jumlah target pada data tanpa outlier\nprint(\"Jumlah data pada tanpa outlier:\")\nprint(target_no_outliers)\n\nJumlah data pada tanpa outlier:\nClassification\n2    64\n1    52\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\nkelas_counts = data['Classification'].value_counts()\n\n# Visualisasi\nplt.figure(figsize=(8, 6))\nkelas_counts.plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Jumlah Sampel untuk Setiap Kelas')\nplt.xlabel('Kelas')\nplt.ylabel('Jumlah Sampel')\nplt.xticks(rotation=0)\nplt.show()"
  },
  {
    "objectID": "main.html#eksplorasi-data",
    "href": "main.html#eksplorasi-data",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "3.6 Eksplorasi Data",
    "text": "3.6 Eksplorasi Data\nMengindentifikasi fitur - fitur yang ada pada dataset kanker payudara dengan tujuan menggunakan seleksi fitur dan menampilkan grafik\n\n3.6.1 Fitur beserta presentase kepentingannya\nmencari skor pada setiap fitur dengan menggunakan metode SelectKBeast dengan mencari nilai mutual information dari setiap fitur.\n\nMUTUAL INFORMATION\n\nMutual information (MI) adalah metrik yang berguna dalam pemilihan fitur karena mengukur seberapa banyak informasi yang saling terkait antara fitur (variabel independen) dengan variabel target (variabel dependen). Dalam konteks pemilihan fitur, kita ingin mempertahankan fitur-fitur yang memiliki hubungan yang kuat atau tinggi dalam menjelaskan variabel target.\nRumus Mutual Information (MI) between X and Y:\n\\[\n\\text{MI}(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log \\left(\\frac{p(x, y)}{p(x) \\cdot p(y)}\\right)\n\\]\nDimana: - MI(X;Y) adalah mutual information antara variabel X dan Y. - p(x,y) adalah probabilitas bersama dari X=x dan Y=y. - p(x) adalah probabilitas margina X=x. - p(y) adalah probabilitas margina Y=y.\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.model_selection import train_test_split\n\n# memisahkan kolom fitur dan target\nfitur = data.drop(columns=['Classification'], axis =1)\ntarget = data['Classification']\n\n# Buat objek SelectKBest dengan mutual_info_classif sebagai fungsi skor\nk_best = SelectKBest(score_func=mutual_info_classif, k='all')  # 'all' berarti akan mempertahankan semua fitur\n\n# Hitung skor fitur\nk_best.fit(fitur, target)\nscores = k_best.scores_\n\n# Dapatkan nama fitur dari kolom data Anda\nfitur_names = fitur.columns\n\n# Tampilkan skor fitur berserta namanya\nfor i, (score, fitur_name) in enumerate(zip(scores, fitur_names)):\n    print(f\"Fitur {i}: {fitur_name}, Skor: {score}\")\n\nFitur 0: Age, Skor: 0.13081809850753978\nFitur 1: BMI, Skor: 0.0\nFitur 2: Glucose, Skor: 0.08175767880050189\nFitur 3: Insulin, Skor: 0.0\nFitur 4: HOMA, Skor: 0.016421497909047345\nFitur 5: Leptin, Skor: 0.0\nFitur 6: Adiponectin, Skor: 0.0\nFitur 7: Resistin, Skor: 0.057669223773980205\nFitur 8: MCP.1, Skor: 0.0\n\n\n\n\n3.6.2 Grafik fitur dan tingkat pentingnya\n\nimport matplotlib.pyplot as plt\n\n# Tampilkan skor fitur dalam grafik\nplt.figure(figsize=(18, 6))\nplt.bar(fitur_names, scores)\nplt.xlabel(\"Nama Fitur\")\nplt.ylabel(\"Skor Fitur\")\nplt.title(\"Skor Fitur SelectKBest\")\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\nKesimpulan :\n\nData tidak memiliki missing values\nData memiliki banyak data redundan\nData memiliki banyak outlier\nPerbandingan proposi data tiap target tidak beda jauh jadi tidak perlu untuk melakukan penyeimbangan data\nHasil skoring fitur masih menggunakan data kotor sehingga perlu difilter kembali"
  },
  {
    "objectID": "main.html#load-dataset-1",
    "href": "main.html#load-dataset-1",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.1 Load Dataset",
    "text": "4.1 Load Dataset\nmeng import/memanggil dataset kanker payudara\n\nimport pandas as pd\n\ndata = pd.read_excel('kankerpayudara.xlsx')\ndata.head(5)\n\n\n\n\n\n\n\n\nAge\nBMI\nGlucose\nInsulin\nHOMA\nLeptin\nAdiponectin\nResistin\nMCP.1\nClassification\n\n\n\n\n0\n48\n23.500000\n70\n2.707\n0.467409\n8.8071\n9.702400\n7.99585\n417.114\n1\n\n\n1\n83\n20.690495\n92\n3.115\n0.706897\n8.8438\n5.429285\n4.06405\n468.786\n1\n\n\n2\n82\n23.124670\n91\n4.498\n1.009651\n17.9393\n22.432040\n9.27715\n554.697\n1\n\n\n3\n68\n21.367521\n77\n3.226\n0.612725\n9.8827\n7.169560\n12.76600\n928.220\n1\n\n\n4\n86\n21.111111\n92\n3.549\n0.805386\n6.6994\n4.819240\n10.57635\n773.920\n1\n\n\n\n\n\n\n\n\n# Rincian dataset (banyak data dan kolom)\n\nprint(\"Banyaknya data : \", data.shape[0])\nprint(\"Banyaknya kolom : \", data.shape[1])\n\nBanyaknya data :  116\nBanyaknya kolom :  10"
  },
  {
    "objectID": "main.html#menghapus-data-duplikat",
    "href": "main.html#menghapus-data-duplikat",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.2 Menghapus Data Duplikat",
    "text": "4.2 Menghapus Data Duplikat\nmenghapus data duplikat yang telah di cari pada eksekusi sebelumnya yang dimana peroleh data duplikat = 0. Dikarenakan data duplikat bernilai 0 pada sisa data tetap dengan jumlah data yang asli\n\n# Menghapus data yang duplikat\ndata_bersih = data.drop_duplicates()\n\nprint(\"Banyaknya sisa data : \", data_bersih.shape[0])\n\nBanyaknya sisa data :  116"
  },
  {
    "objectID": "main.html#menghapus-outlier",
    "href": "main.html#menghapus-outlier",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.3 Menghapus Outlier",
    "text": "4.3 Menghapus Outlier\n\n# Menghapus data outlier dari DataFrame 'data'\ndata_cleaned = data.drop(outlier_indices)\n\n# Menampilkan DataFrame setelah menghapus outlier\nprint(\"Data setelah menghapus outlier:\")\n\n# Rincian dataset (banyak data dan kolom)\nprint(\"Banyaknya data : \", data_cleaned.shape[0])\n\nData setelah menghapus outlier:\nBanyaknya data :  104"
  },
  {
    "objectID": "main.html#menyeimbangkan-data-tiap-target",
    "href": "main.html#menyeimbangkan-data-tiap-target",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.4 Menyeimbangkan Data Tiap Target",
    "text": "4.4 Menyeimbangkan Data Tiap Target\nPada hasil Mengidentifikasi jumlah data di peroleh jumlah target data 64 : 52. Namun setelah di lakukan Penghapusan Outlier maka di peroleh nilai sebagai berikut:\n\nfitur = data_cleaned.drop(columns=['Classification'])\ntarget = data_cleaned['Classification']\n\ntarget.value_counts()\n\nClassification\n2    57\n1    47\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\n# Visualisasi distribusi kelas\nplt.figure(figsize=(6, 4))\ntarget.value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Distribusi Kelas')\nplt.xlabel('Kelas')\nplt.ylabel('Jumlah Sampel')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\ndikarenakan data classification tidak mengalami ketidakeseimabangan data, maka tidak perlu melakukan penyeimbangan data target."
  },
  {
    "objectID": "main.html#eksplorasi-data-skoring-fitur",
    "href": "main.html#eksplorasi-data-skoring-fitur",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.5 Eksplorasi Data (Skoring Fitur)",
    "text": "4.5 Eksplorasi Data (Skoring Fitur)\nMengindentifikasi fitur - fitur yang ada pada dataset kanker payudara dengan tujuan menggunakan seleksi fitur dan menampilkan grafik\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\nfitur = data_cleaned.drop(columns=['Classification'], axis =1)\ntarget = data_cleaned['Classification']\n# Buat objek SelectKBest dengan mutual_info_classif sebagai fungsi skor\nk_best = SelectKBest(score_func=mutual_info_classif, k='all')  # 'all' berarti akan mempertahankan semua fitur\n\n# Hitung skor fitur\nk_best.fit(fitur, target)\nscores = k_best.scores_\n\n# Dapatkan nama fitur dari kolom data Anda\nfitur_names = fitur.columns\n\n# Tampilkan skor fitur berserta namanya\nfor i, (score, fitur_name) in enumerate(zip(scores, fitur_names)):\n    print(f\"Fitur {i}: {fitur_name}, Skor: {score}\")\n    \n\nFitur 0: Age, Skor: 0.12297753765165353\nFitur 1: BMI, Skor: 0.0\nFitur 2: Glucose, Skor: 0.10980717436109777\nFitur 3: Insulin, Skor: 0.0\nFitur 4: HOMA, Skor: 0.027261437776825215\nFitur 5: Leptin, Skor: 0.0\nFitur 6: Adiponectin, Skor: 0.016256470521858413\nFitur 7: Resistin, Skor: 0.05710392435008105\nFitur 8: MCP.1, Skor: 0.0\n\n\nSetelah dilakukan pencarian skor pada setiap fitur, terdapat 4 fitur yang tidak memiliki skor atau bernilai 0 dan terdapat 5 fitur yang memilki nilai. maka dari itu, selanjutnya akan di lakukan penghapusan fitur yang tidak memiliki nilai.\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\n\nk_best = SelectKBest(score_func=mutual_info_classif, k='all')  # 'all' berarti akan mempertahankan semua fitur\n\n# Hitung skor fitur\nk_best.fit(fitur, target)\nscores = k_best.scores_\n\n# Dapatkan nama fitur dari kolom data Anda\nfitur_names = fitur.columns\n\nindeks_fitur_hapus = [i for i, score in enumerate(scores) if score == 0]\n\n# Tampilkan skor fitur berserta namanya\nfor i, (score, fitur_name) in enumerate(zip(scores, fitur_names)):\n    print(f\"Fitur {i}: {fitur_name}, Skor: {score}\")\n    \n\n# Dapatkan indeks fitur yang memiliki skor 0\n\n# Buang fitur-fitur yang memiliki skor 0 dari DataFrame fitur\nfitur = fitur.drop(fitur.columns[indeks_fitur_hapus], axis=1)\n\nFitur 0: Age, Skor: 0.1597212159818704\nFitur 1: Glucose, Skor: 0.06071687672575954\nFitur 2: HOMA, Skor: 0.027261437776825215\nFitur 3: Adiponectin, Skor: 0.016256470521858413\nFitur 4: Resistin, Skor: 0.05710392435008105\n\n\n\nimport matplotlib.pyplot as plt\n\n# Tampilkan skor fitur dalam grafik\nplt.figure(figsize=(18, 6))\nplt.bar(fitur_names, scores)\nplt.xlabel(\"Nama Fitur\")\nplt.ylabel(\"Skor Fitur\")\nplt.title(\"Skor Fitur SelectKBest\")\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\ndataset_baru = pd.concat([fitur, target], axis=1)\n\n# Display the combined table\nprint(dataset_baru)\n\n     Age  Glucose      HOMA  Adiponectin  Resistin  Classification\n0     48       70  0.467409     9.702400   7.99585               1\n1     83       92  0.706897     5.429285   4.06405               1\n2     82       91  1.009651    22.432040   9.27715               1\n3     68       77  0.612725     7.169560  12.76600               1\n4     86       92  0.805386     4.819240  10.57635               1\n..   ...      ...       ...          ...       ...             ...\n110   54      119  3.495982     8.010000   5.06000               2\n111   45       92  0.755688    12.100000  10.96000               2\n112   62      100  1.117400    21.420000   7.32000               2\n113   65       97  1.370998    22.540000  10.33000               2\n114   72       82  0.570392    33.750000   3.27000               2\n\n[104 rows x 6 columns]"
  },
  {
    "objectID": "main.html#split-data",
    "href": "main.html#split-data",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.6 Split Data",
    "text": "4.6 Split Data\nproses memisahkan dataset menjadi dua atau lebih bagian yang berbeda. Tujuan umumnya adalah untuk menggunakan satu bagian data sebagai data pelatihan (training data) untuk melatih model, dan bagian lainnya sebagai data pengujian (testing data) untuk menguji kinerja model.\n\nimport pandas as pd\n\n# Menyimpan DataFrame ke dalam file CSV\ndataset_baru.to_excel('dataset_baru.xlsx', index=False)\n\n\nfrom sklearn.model_selection import train_test_split\n\n# melakukan pembagian dataset, dataset dibagi menjadi 80% data training dan 20% data testing\nfitur_train, fitur_test, target_train, target_test = train_test_split(fitur, target, test_size = 0.2, random_state=42)"
  },
  {
    "objectID": "main.html#normalisasi-data",
    "href": "main.html#normalisasi-data",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.7 Normalisasi Data",
    "text": "4.7 Normalisasi Data\nproses pengubahan nilai-nilai dalam suatu dataset menjadi rentang skala tertentu atau memastikan bahwa nilai-nilai tersebut mengikuti distribusi yang dapat meningkatkan kinerja beberapa algoritma machine learning.\n\n4.7.1 Menggunakan Standarscaler (zscore)\nNormalisasi menggunakan Z-Score atau Standard Scaler adalah salah satu teknik normalisasi yang umum digunakan dalam pengolahan data dan machine learning. Normalisasi ini mengubah setiap nilai dalam dataset sehingga memiliki rata-rata nol dan deviasi standar satu. Hal ini membantu untuk menghilangkan perbedaan skala antar fitur, membuat data lebih mudah diinterpretasikan, dan meningkatkan performa beberapa algoritma machine learning. Berikut adalah penjelasan lebih detail tentang Normalisasi Standar atau Z-Score:\nLangkah-langkah Normalisasi Standar (Z-Score):\n\nHitung Rata-rata dan Deviasi Standar: Untuk setiap fitur dalam dataset, hitung rata-rata \\(\\mu\\) dan deviasi standar \\(\\sigma\\).\n\\[ \\mu = \\frac{1}{N}\\sum_{i=1}^{N} X_i \\]\n\\[ \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu)^2} \\]\nDi sini, \\(N\\) adalah jumlah total sampel, dan \\(X_i\\) adalah nilai individu dalam suatu fitur.\nNormalisasi Setiap Nilai: Untuk setiap nilai dalam setiap fitur, normalisasikan nilai tersebut menggunakan rumus Z-Score:\n\\[ Z = \\frac{X - \\mu}{\\sigma} \\]\nDi sini, \\(X\\) adalah nilai individu, \\(\\mu\\) adalah rata-rata fitur, dan \\(\\sigma\\) adalah deviasi standar fitur.\n\nBerikut adalah contoh penggunaan metode Standard Scaling (Z-Score Normalization) pada data kolom X yang berbeda:\nDiberikan tabel dengan kolom X berikut:\n\n\n\nX\nX’\n\n\n\n\n8\n0\n\n\n20\n0\n\n\n12\n0\n\n\n15\n0\n\n\n25\n0\n\n\n\nLangkah-langkah untuk melakukan normalisasi dengan metode Standard Scaling:\n\nHitung rata-rata (mean) dan standar deviasi (standard deviation) dari kolom X.\n\nRata-rata (mean) = \\(\\frac{8 + 20 + 12 + 15 + 25}{5} = 16\\)\nStandar Deviasi = \\[ \\sqrt{\\frac{\\sum{(X_i - \\text{mean})^2}}{N}} = \\sqrt{\\frac{(8-16)^2 + (20-16)^2 + (12-16)^2 + (15-16)^2 + (25-16)^2}{5}} \\approx 5.477\\]\n\nNormalisasikan setiap nilai dalam kolom X menggunakan rumus Z-Score Normalization: \\[ X' = \\frac{X - \\text{mean}(X)}{\\text{std}(X)} \\]\nSehingga, nilai X’ untuk setiap baris dapat dihitung sebagai berikut:\n\n\\[{X'}_1 = \\frac{8 - 16}{5.477} \\approx -1.46 \\]\n\\[{X'}_2 = \\frac{20 - 16}{5.477} \\approx 0.73 \\]\n\\[{X'}_3 = \\frac{12 - 16}{5.477} \\approx -0.73 \\]\n\\[{X'}_4 = \\frac{15 - 16}{5.477} \\approx -0.22 \\]\n\\[{X'}_5 = \\frac{25 - 16}{5.477} \\approx 1.64 \\]\n\nSehingga, hasil normalisasi (X’) untuk contoh ini adalah:\n\n\n\n\nX\nX’\n\n\n\n\n8\n-1.46\n\n\n20\n0.73\n\n\n12\n-0.73\n\n\n15\n-0.22\n\n\n25\n1.64\n\n\n\nIni adalah contoh penggunaan Standard Scaling pada data kolom X yang berbeda.\n\nimport pickle\nfrom sklearn.preprocessing import StandardScaler\n\n# membuat dan melatih objek StandardScaler\nzscore_scaler = StandardScaler()\nzscore_scaler.fit(fitur_train)\n\nwith open('zscorescaler_baru.pkl', 'wb') as file:\n    pickle.dump(zscore_scaler, file)\n# menerapkan normalisasi zscore pada data training\nzscore_training = zscore_scaler.transform(fitur_train)\n\n# menerapkan normalisasi zscore pada data testing\nzscore_testing = zscore_scaler.transform(fitur_test)\n\n\n\n4.7.2 Menggunakan Minmaxscaler\nMinMax Normalization atau MinMax Scaling digunakan untuk mengubah nilai-nilai dalam suatu fitur ke dalam rentang tertentu, biasanya antara 0 dan 1. MinMax Normalization mengubah setiap nilai X dalam fitur ke dalam rentang yang diinginkan menggunakan rumus berikut:\nLangkah-langkah Normalisasi Minmax:\n\nIdentifikasi Rentang: Tentukan rentang nilai yang ingin Anda gunakan. Biasanya, dalam Min-Max Scaling, rentang nilai yang dipilih adalah 0 hingga 1, tetapi ini bisa disesuaikan tergantung pada kasus penggunaan.\nHitung Nilai Minimum dan Maksimum: Tentukan nilai minimum (min) dan nilai maksimum (max) dari setiap fitur dalam kumpulan data yang akan dinormalisasi.\nNormalisasi: Gunakan formula rumusn Min-Max Scaling untuk mengubah nilai-nilai dalam rentang yang ditentukan.\n\nRumus Minmax Scaler\n\\[X' = \\frac{X-Xmin}{Xmax - Xmin}\\]\nDimana : - X adalah nilai asli dari suatu kolom/fitur - min adalah nilai minimum dari suatu kolom/fitur dalam dataset - max adalah nilai maximum dari suatu kolom/fitur dalam dataset - X’ adalah nilai X yang telah dinormalisasi.\nBerikut adalah contoh penggunaan Min-Max Scaling pada data kolom X yang berbeda:\nDiberikan tabel dengan kolom X berikut:\n\n\n\nX\nX’\n\n\n\n\n5\n0\n\n\n15\n0\n\n\n8\n0\n\n\n20\n0\n\n\n\nUntuk melakukan normalisasi dengan Min-Max Scaling, kita perlu mengidentifikasi nilai terendah dan tertinggi pada kolom. Dalam kasus ini:\n\nNilai terendah pada kolom X (min) = 5\nNilai tertinggi pada kolom X (max) = 20\n\nSehingga, nilai X’ hasil normalisasi dapat dihitung seperti berikut:\n\n\n\nX\nX’\n\n\n\n\n5\n(5 - 5) / (20 - 5) = 0\n\n\n15\n(15 - 5) / (20 - 5) = 0.6667\n\n\n8\n(8 - 5) / (20 - 5) = 0.3333\n\n\n20\n(20 - 5) / (20 - 5) = 1\n\n\n\nJadi, nilai X’ hasil normalisasi untuk contoh ini adalah:\n\n\n\nX\nX’\n\n\n\n\n5\n0\n\n\n15\n0.6667\n\n\n8\n0.3333\n\n\n20\n1\n\n\n\nIni adalah contoh penggunaan Min-Max Scaling pada data kolom X yang berbeda.\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# membuat dan melatih objek MinMaxScaler\nminmaxscaler = MinMaxScaler()\nminmaxscaler.fit(fitur_train)\n\n# menerapkan normalisasi zscore pada data training\nminmax_training = minmaxscaler.transform(fitur_train)\n\n# menerapkan normalisasi zscore pada data testing\nminmax_testing = minmaxscaler.transform(fitur_test)"
  },
  {
    "objectID": "main.html#modeling",
    "href": "main.html#modeling",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.8 4. MODELING",
    "text": "4.8 4. MODELING\nSetelah dilakukan skenario perulangan untuk menghasilkan model terbaik, dapat dikeathui bahwasannya model klasifikasi yang terbaik untuk data anggur merah ini adalah dengan menggunakan : - Metode Random Forest - Metode normalisasi nya adalah Z-score Scaler - Banyak Fitur yang digunakan dalam data sebanyak 5 fitur - Parameter dalam metode yang digunakan, sebagai berikut:\n\n\njumlah estimator :\nmaksimal kedalaman :\nminimal pembagian sampel :\nminimal sampel daun :\n\n\n\n4.8.1 Menggunakan Random Forest\nandom Forest adalah algoritma pembelajaran terawasi yang digunakan untuk tugas klasifikasi dan regresi dalam machine learning. Ini merupakan bagian dari keluarga algoritma yang dikenal sebagai ensemble learning, yang menggabungkan hasil beberapa model untuk meningkatkan kinerja dan ketepatan prediksi.\nKonsep inti dari Random Forest adalah membuat sejumlah besar pohon keputusan saat melakukan prediksi. Setiap pohon keputusan dibuat berdasarkan sampel acak dari data pelatihan dan fitur yang dipilih secara acak. Proses ini mengurangi risiko overfitting (memfitting data pelatihan secara berlebihan) yang sering terjadi pada pohon keputusan tunggal.\nSelama proses pelatihan, setiap pohon keputusan dalam hutan acak memilih subset data yang diambil secara acak dan subset fitur untuk membuat keputusan. Ketika melakukan prediksi, setiap pohon memberikan hasilnya, dan hasil akhir dari Random Forest diperoleh dengan mengambil mayoritas suara dari semua pohon keputusan (untuk klasifikasi) atau rerata hasil (untuk regresi).\nKelebihan dari Random Forest termasuk kemampuannya dalam menangani data yang besar dengan fitur yang banyak, serta kemampuan untuk mengatasi overfitting. Namun, seperti halnya dengan banyak algoritma machine learning, pengaturan parameter yang tidak tepat atau kekurangan pemrosesan data yang tepat dapat mempengaruhi kinerja Random Forest.\n\nLangkah-Langkah Random Forest:\n1. Pembuatan Bootstrap Samples: - Buat beberapa dataset bootstrap dari dataset pelatihan dengan pengambilan sampel dengan pengembalian.\n2. Pembuatan Pohon: - Bangun pohon keputusan untuk setiap dataset bootstrap. Pohon ini dibangun dengan memilih fitur secara acak pada setiap split.\n3. Prediksi dari Setiap Pohon: - Lakukan prediksi pada setiap pohon untuk data uji.\n4. Klasifikasi (Voting) atau Regresi (Average): - Untuk klasifikasi, tentukan hasil akhir menggunakan voting mayoritas. Untuk regresi, ambil rata-rata prediksi dari semua pohon.\n5. Evaluasi Kinerja: - Evaluasi kinerja model menggunakan metrik yang sesuai dengan tugas (misalnya, akurasi untuk klasifikasi, MSE untuk regresi).\nRumus:\n\n\n4.8.1.1 Gini Index (Untuk Pohon Keputusan):\n\\[ Gini(t) = 1 - \\sum_{i=1}^{c} (p_i)^2 \\]\n\n\n4.8.1.2 Prediksi Klasifikasi (Voting):\n\\[ \\text{Prediction} = \\text{argmax}(\\text{votes}) \\]\n\n\n4.8.1.3 Prediksi Regresi (Average):\n\\[ \\text{Prediction} = \\frac{1}{N} \\sum_{i=1}^{N} y_i \\]\nDi sini, \\(t\\) adalah node dalam pohon, \\(c\\) adalah jumlah kelas, \\(p_i\\) adalah proporsi sampel di kelas \\(i\\), \\(N\\) adalah jumlah pohon dalam ensemble, dan \\(y_i\\) adalah prediksi pohon ke-\\(i\\).\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nbest_accuracy_rf_zscore = 0\nbest_k_zscore = 0\nbest_accuracy_rf_minmax = 0\nbest_k_minmax = 0\n\nfor k in range(1, fitur_train.shape[1] + 1):\n    # Buat objek SelectKBest dengan mutual_info_classif sebagai fungsi skor\n    k_best = SelectKBest(score_func=mutual_info_classif, k=k)\n\n    # Fiturkan objek SelectKBest ke data training untuk kedua normalisasi (zscore dan minmax)\n    zscore_training_terbaik = k_best.fit_transform(zscore_training, target_train)\n    zscore_testing_terbaik = k_best.transform(zscore_testing)\n\n    # Transformasi data testing dengan objek SelectKBest yang sudah difit ke data training\n    minmaxtesting_terbaik = k_best.transform(minmax_testing)\n\n    # Buat dan latih model dengan normalisasi zscore\n    model_zscore = RandomForestClassifier(random_state=42)\n    model_zscore.fit(zscore_training_terbaik, target_train)\n\n    # Lakukan prediksi pada data uji dengan normalisasi zscore\n    y_pred_rf_zscore = model_zscore.predict(zscore_testing_terbaik)\n\n    # Hitung akurasi dengan normalisasi zscore\n    accuracy_rf_zscore = accuracy_score(target_test, y_pred_rf_zscore)\n\n    # Buat dan latih model dengan normalisasi minmax\n    model_minmax = RandomForestClassifier(random_state=42)\n    model_minmax.fit(zscore_training_terbaik, target_train)  # Gunakan zscore_training_terbaik untuk minmax\n\n    # Lakukan prediksi pada data uji dengan normalisasi minmax\n    y_pred_rf_minmax = model_minmax.predict(minmaxtesting_terbaik)\n\n    # Hitung akurasi dengan normalisasi minmax\n    accuracy_rf_minmax = accuracy_score(target_test, y_pred_rf_minmax)\n\n    # Memeriksa apakah akurasi dengan normalisasi zscore lebih baik dari yang sebelumnya\n    if accuracy_rf_zscore &gt; best_accuracy_rf_zscore:\n        best_accuracy_rf_zscore = accuracy_rf_zscore\n        best_k_zscore = k\n\n    # Memeriksa apakah akurasi dengan normalisasi minmax lebih baik dari yang sebelumnya\n    if accuracy_rf_minmax &gt; best_accuracy_rf_minmax:\n        best_accuracy_rf_minmax = accuracy_rf_minmax\n        best_k_minmax = k\n\nprint(\"Dengan Normalisasi Zscore:\")\nprint(\"Fitur terbaik yang bisa digunakan\", best_k_zscore, \"dengan akurasi : \", best_accuracy_rf_zscore)\n\nprint(\"Dengan Normalisasi Minmax:\")\nprint(\"Fitur terbaik yang bisa digunakan\", best_k_minmax, \"dengan akurasi : \", best_accuracy_rf_minmax)\n\nDengan Normalisasi Zscore:\nFitur terbaik yang bisa digunakan 3 dengan akurasi :  0.8571428571428571\nDengan Normalisasi Minmax:\nFitur terbaik yang bisa digunakan 2 dengan akurasi :  0.5238095238095238\n\n\nPada percobaan dengan menggunakan metode Random Forest terhadap dataset kanker payudara diperoleh hasil akurasi dari setiap Normalisasi Z-score = 0.857 dan Minmax = 0.523\nmaka dari itu, untuk selanjutnya kita akan menggunakan normalisasi Z-score untuk membuat modelnya.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n\n# Define the parameter grid for Random Forest\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # You can adjust the number of trees\n    'max_depth': [None, 10, 20, 30],  # You can adjust the maximum depth of each tree\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Create a Random Forest model\nrandom_forest = RandomForestClassifier()\n\n# MINMAX\ngrid_search2 = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search2.fit(zscore_training, target_train)\nprint(\"Best Parameters MINMAX:\", grid_search2.best_params_)\nbest_n_estimators_zscore = grid_search2.best_params_['n_estimators']\nbest_max_depth_zscore = grid_search2.best_params_['max_depth']\nbest_min_samples_split_zscore = grid_search2.best_params_['min_samples_split']\nbest_min_samples_leaf_zscore = grid_search2.best_params_['min_samples_leaf']\n\nBest Parameters MINMAX: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n\n\n\n# MINMAX\nmodel_rf_zscore = RandomForestClassifier( max_depth= best_max_depth_zscore,min_samples_leaf= best_min_samples_leaf_zscore,min_samples_split= best_min_samples_split_zscore, n_estimators= best_n_estimators_zscore)\nmodel_rf_zscore.fit(zscore_training, target_train)\n# Lakukan prediksi pada data uji dengan normalisasi zscore\ny_pred_zscore = model_rf_zscore.predict(zscore_testing)\n# Hitung akurasi dengan normalisasi zscore\naccuracy_rf_zscore = accuracy_score(target_test, y_pred_zscore)\n\nprint(\"AKURASI RANDOM FOREST\")\nprint(\"AKURASI ZSCORE :\",accuracy_rf_zscore)\n\nAKURASI RANDOM FOREST\nAKURASI ZSCORE : 0.7619047619047619\n\n\n\nimport pickle\n\npath_rf = 'gridrandomforestzscore.pkl'\nwith open(path_rf, 'wb') as model_file:\n    pickle.dump(model_rf_zscore, model_file)\n\n\n\n\n4.8.2 —EVALUASI MODEL—\nPada tahap ini model terbaik yang diperoleh pada tahap modeling dilakukan validasi dengan menampilkan nilai confusion matrix nya atau laporan klasifikasinya dengan menggunakan grafik ROC-AUC\n\n\n4.8.3 CONFUSION MATRIX\n\nConfusion matrix adalah sebuah tabel yang digunakan dalam evaluasi kinerja model klasifikasi untuk memahami performa model dalam memprediksi kelas-kelas target. Matrix ini memiliki empat sel yang mewakili:\n\nTrue Positive (TP): Prediksi yang benar ketika kelas sebenarnya adalah positif.\nTrue Negative (TN): Prediksi yang benar ketika kelas sebenarnya adalah negatif.\nFalse Positive (FP): Prediksi yang salah ketika model memprediksi positif tetapi kelas sebenarnya negatif (juga dikenal sebagai Type I error).\nFalse Negative (FN): Prediksi yang salah ketika model memprediksi negatif tetapi kelas sebenarnya positif (juga dikenal sebagai Type II error).\n\nBentuk dari tabel Confusion Matrix\n\n\n\n\nPredicted Negative\nPredicted Positive\n\n\n\n\nActual Negative\nTrue Negative (TN)\nFalse Positive (FP)\n\n\nActual Positive\nFalse Negative (FN)\nTrue Positive (TP)\n\n\n\nDari Confusion Matriks, kta dapat menghitung metrik evaluasi seperti akurasi, presisi, recall, F1-score, dan lainnya yang membantu dalam mengevaluasi performa model klasifikasi.\n\n4.8.3.1 Metrik Evaluasi\nMetrik evaluasi adalah ukuran atau parameter yang digunakan untuk mengevaluasi kinerja suatu model atau sistem dalam melakukan tugas tertentu, seperti klasifikasi, regresi, atau tugas lainnya dalam bidang machine learning dan statistika. Metrik-metrik ini membantu dalam memahami seberapa baik atau buruk model tersebut dalam melakukan prediksi atau tugas yang ditetapkan.\nBeberapa metrik evaluasi umum dalam machine learning termasuk: &gt; - Akurasi (Accuracy): Seberapa sering model memberikan prediksi yang benar secara keseluruhan. Rumus Akurasi : \\[ Accuracy = \\frac{TN + TP}{TN + FP + FN + TP} \\] &gt; - Presisi (Precision): Proporsi dari prediksi positif yang benar dibandingkan dengan semua prediksi positif yang dibuat oleh model Rumus Precision : \\[ Precision = \\frac{TP}{TP + FP} \\] &gt; - Recall (Sensitivity atau True Positive Rate): Proporsi dari kelas positif yang diprediksi dengan benar oleh model. Rumus Recall : \\[ Recall = \\frac{TP}{TP + FN} \\] &gt; - F1-Score: Nilai rata-rata harmonik antara presisi dan recall. Berguna ketika perlu menyeimbangkan antara presisi dan recall. Rumus F1-Score : \\[ F1-Score = 2 x \\frac {Presisi x Recall}{Presisi x Recall} \\] &gt; - Specificity (Specificity atau True Negative Rate): Proporsi dari kelas negatif yang diprediksi dengan benar oleh model. Rumus Specificity : \\[ Specificity = \\frac{TN}{TN + FP} \\]\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(y_true, y_pred, model_name, scaler_name):\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n\n    print(f\"\\nModel {model_name} Menggunakan {scaler_name}:\")\n    print(f'Akurasi: {accuracy:.2f}')\n    print(f'Presisi: {precision:.2f}')\n    print(f'Recall: {recall:.2f}')\n    print(f'F1-Score: {f1:.2f}')\n\n# Evaluasi model Random Forest dengan zscorescaler\nevaluate_model(target_test, y_pred_rf_zscore, \"Random Forest\", \"zscoreScaler\")\n\n# Evaluasi model Random Forest dengan minmaxscaler\nevaluate_model(target_test, y_pred_rf_minmax, \"Random Forest\", \"minmaxScaler\")\n\n\nModel Random Forest Menggunakan zscoreScaler:\nAkurasi: 0.81\nPresisi: 0.82\nRecall: 0.81\nF1-Score: 0.81\n\nModel Random Forest Menggunakan minmaxScaler:\nAkurasi: 0.52\nPresisi: 0.27\nRecall: 0.52\nF1-Score: 0.36\n\n\nc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score,roc_curve\n\n# Evaluasi model dengan data uji Z-score\nprint(\"\\nEVALUASI MODEL DENGAN DATA UJI Z-score\")\nprint(\"Confusion Matrix Z-score:\")\nconf_matrix = confusion_matrix(target_test, y_pred_rf_zscore)\nprint(conf_matrix)\n\n# Mendapatkan nilai TP, TN, FP, FN dari confusion matrix\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\nFN = conf_matrix[1, 0]\nTP = conf_matrix[1, 1]\n\nprint(\"\\nTrue Positive (TP):\", TP)\nprint(\"True Negative (TN):\", TN)\nprint(\"False Positive (FP):\", FP)\nprint(\"False Negative (FN):\", FN)\n\n\nEVALUASI MODEL DENGAN DATA UJI Z-score\nConfusion Matrix Z-score:\n[[ 7  3]\n [ 1 10]]\n\nTrue Positive (TP): 10\nTrue Negative (TN): 7\nFalse Positive (FP): 3\nFalse Negative (FN): 1\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Visualisasi Confusion Matrix\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n\n\n\n\nprint(\"\\nClassification Report Z-score:\")\nprint(classification_report(target_test, y_pred_rf_zscore))\nprint(\"ROC-AUC Score Z-score:\", roc_auc_score(target_test, y_pred_rf_zscore))\n\n\nClassification Report Z-score:\n              precision    recall  f1-score   support\n\n           1       0.88      0.70      0.78        10\n           2       0.77      0.91      0.83        11\n\n    accuracy                           0.81        21\n   macro avg       0.82      0.80      0.81        21\nweighted avg       0.82      0.81      0.81        21\n\nROC-AUC Score Z-score: 0.8045454545454546\n\n\nAda beberapa poin yang perlu diperhatikan : 1. Akurasi 2. Presisi mengukur sejauh mana hasil positif yang diprediksi oleh model adalah benar. 3. Recall Recall mengukur sejauh mana model dapat mengidentifikasi dengan benar semua instance positif dalam data. 4. F1-Score adalah metrik gabungan yang mempertimbangkan presisi dan recall.\nDengan mempertimbangkan keempat poin di atas, diambil keputusan akan dilakukan modelling menggunakan support vector machine dengan normalisasi minmaxscaler"
  },
  {
    "objectID": "main.html#grafik-roc-auc",
    "href": "main.html#grafik-roc-auc",
    "title": "1  TUGAS KLASIFIKASI DATA PROYEK SAINS DATA - B",
    "section": "4.9 ### GRAFIK ROC-AUC",
    "text": "4.9 ### GRAFIK ROC-AUC\nMetrik evaluasi ROC (Receiver Operating Characteristic) dan AUC (Area Under the ROC Curve) adalah alat evaluasi yang digunakan untuk mengukur kinerja model klasifikasi, terutama ketika model harus mengklasifikasikan antara dua kelas.\n\n4.9.0.1 Receiver Operating Characteristic (ROC) Curve\nROC Curve adalah adalah kurva grafik yang menampilkan kinerja model klasifikasi pada berbagai tingkat cutoff (threshold) untuk membedakan antara kelas positif dan negatif. Didalam ROC kurva dapat diketahui sensitivity (True Positive Rate) dan False Positive Rate (1-Specificity), untuk menunjukkan seberapa baik model klasifikasi sehingga dapat membedakan antara kelas positif dan negatif.\n\n\n4.9.0.2 Area Under the ROC Curve (AUC-ROC):\nAUC-ROC adalah ukuran dari luas area di bawah kurva ROC. - Interpretasi : Nilai AUC berkisar antara 0 hingga 1. Semakin dekat nilainya ke 1, semakin baik model dalam membedakan antara kelas positif dan negatif. Jika nilainya 0.5, itu menunjukkan klasifikasi acak.\n\n# Kurva ROC-AUC untuk model dengan data uji z-score\nfpr_zscore, tpr_zscore, thresholds_zscore = roc_curve(target_test, y_pred_rf_zscore, pos_label=2)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_zscore, tpr_zscore, label='ROC Curve Z-score (AUC = %0.2f)' % roc_auc_score(target_test, y_pred_rf_zscore))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve Z-score')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]